{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'G:/dl-pytorch/2_object_detection/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "exec(open(\"Inference/inference.py\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from models.model import SSD\n",
    "from lib  import *\n",
    "from data.data_loader.transform import DataTransform\n",
    "from data.data_loader.cfg import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = SSD(phase=\"inference\", cfg=cfg)\n",
    "net_weights = torch.load(\"G:/dl-pytorch/2_object_detection/data/weights/ssd300_100.pth\", map_location={\"cuda:0\":\"cpu\"})\n",
    "net.load_state_dict(net_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    color_mean = (104, 117, 123)\n",
    "    input_size = 300\n",
    "    transform = DataTransform(input_size, color_mean)\n",
    "\n",
    "    phase = \"val\"\n",
    "    img_tranformed, boxes, labels = transform(frame, phase, \"\", \"\") \n",
    "\n",
    "    img_tensor = torch.from_numpy(img_tranformed[:,:,(2,1,0)]).permute(2,0,1)\n",
    "\n",
    "    net.eval()\n",
    "    input = img_tensor.unsqueeze(0) #(1, 3, 300, 300)\n",
    "    output = net(input)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    colors = [(255,0,0), (0,255,0), (0,0,255)]\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    detections = output.data #(1, 21, 200, 5) 5: score, cx, cy, w, h\n",
    "    scale = torch.Tensor(frame.shape[1::-1]).repeat(2)\n",
    "\n",
    "    for i in range(detections.size(1)):\n",
    "        j = 0\n",
    "        while detections[0, i, j, 0] >= 0.6:\n",
    "            score = detections[0, i, j, 0]\n",
    "            pt = (detections[0, i, j, 1:]*scale).cpu().numpy()\n",
    "            cv2.rectangle(frame,\n",
    "                          (int(pt[0]), int(pt[1])),\n",
    "                          (int(pt[2]), int(pt[3])),\n",
    "                          colors[i%3], 2\n",
    "                          )\n",
    "            display_text = \"%s: %.2f\"%(classes[i-1], score)\n",
    "            cv2.putText(frame, display_text, (int(pt[0]), int(pt[1])),\n",
    "                font, 0.5, (200, 69, 255), 1, cv2.LINE_AA)\n",
    "            j += 1\n",
    "    cv2.imshow('frame',frame)\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "83e04796f542b7753d232924b38a9a2cfb159c777544a5715b7f8b3f2fa55f1f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch_envs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
